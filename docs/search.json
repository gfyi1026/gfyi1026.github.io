[
  {
    "objectID": "premierleague.html",
    "href": "premierleague.html",
    "title": "Premier League 2021–22",
    "section": "",
    "text": "Dataset page: TidyTuesday: Premier League Match Data\nOriginal source: Official Premier League Statistics\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-04-04/soccer21-22.csv')\n\nteam_goals &lt;- soccer |&gt;\n  group_by(HomeTeam) |&gt;\n  summarise(avg_goals = mean(FTHG, na.rm = TRUE))\n\nggplot(team_goals, aes(x = HomeTeam, y = avg_goals)) +\n  geom_col(fill = \"blue\") +\n  labs(\n    title = \"Average Home Goals per Match by Team (2021–22)\",\n    x = \"Team\",\n    y = \"Average Goals\",\n    caption = \"Source: TidyTuesday 2023-04-04, Premier League 2021–22\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my first project for my intro to data science class.\nThis is me playing around with R and my website\n\n1 + 2\n\n[1] 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guy Fuchs",
    "section": "",
    "text": "Hi there, my name is Guy Fuchs and I am a junior at Pomona College studying computer science and physics. I also play on the soccer team here, and love traveling."
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "",
    "text": "For this project I want to explore how police stop patterns differ across several cities included in the Stanford Open Policing Project. I plan to pull data from three separate city-level tables (Long Beach in CA, Mesa in AZ, and San Jose in CA) and use SQL to compare how many pedestrian and vehicular stops occur over time, how the racial mix of the people stopped varies, and how often citations are issued across race. My plan is to combine the data sets, summarize and analyze them using different SQL queries, and then visualize the results with plots that will highlight differences across cities and stop types. My main goal is to see whether the data suggests meaningful variations in who gets stopped or cited, and whether those patterns look similar or different across the three cities.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DBI)\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)"
  },
  {
    "objectID": "project5.html#introduction",
    "href": "project5.html#introduction",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "",
    "text": "For this project I want to explore how police stop patterns differ across several cities included in the Stanford Open Policing Project. I plan to pull data from three separate city-level tables (Long Beach in CA, Mesa in AZ, and San Jose in CA) and use SQL to compare how many pedestrian and vehicular stops occur over time, how the racial mix of the people stopped varies, and how often citations are issued across race. My plan is to combine the data sets, summarize and analyze them using different SQL queries, and then visualize the results with plots that will highlight differences across cities and stop types. My main goal is to see whether the data suggests meaningful variations in who gets stopped or cited, and whether those patterns look similar or different across the three cities.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DBI)\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)"
  },
  {
    "objectID": "project5.html#first-query",
    "href": "project5.html#first-query",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "First Query",
    "text": "First Query\nFor my first query, my goal is to combine three city tables and keep only vehicular/pedestrian, extract the year, and then count stops by city-year-type. I restrict the year span to be the years 2014 through 2016, which is the period that Mesa, Long Beach, and San Jose all report data in the SOPP database.\n\nSELECT\n  city,\n  YEAR(date) AS year,\n  type,\n  COUNT(*) AS n_stops\nFROM (\n\n    SELECT 'Mesa' AS city, date, type\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'Long Beach' AS city, date, type\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, type\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) &gt;= 2014\n    AND YEAR(date) &lt;= 2016\n    AND type IN ('vehicular', 'pedestrian')\nGROUP BY\n    city, year, type\nORDER BY\n    city, year, type;\n\nNow, I can use the output of the above query in my plot below:\n\nggplot(stops_by_year_type |&gt;\n         mutate(n_stops = as.numeric(n_stops)), \n       aes(x = year, y = n_stops, color = type)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ city) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    title = \"Traffic and Pedestrian Stops Over Time\",\n    x = \"Year\",\n    y = \"Number of stops\",\n    color = \"Stop type\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n  axis.text.x = element_text(angle = 45, hjust = 1),\n  panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nThis plot shows how the number of traffic stops changes over time for Long Beach, Mesa, and San Jose. I focused on pedestrian and vehicular stops between 2014 and 2016 so that all three cities share a common window of data, and we can see that vehicular stops dominate the totals in every city, and the overall levels differ quite a bit across the three locations. The trends themselves are also different, which gives the idea that each department operates under its own patterns and volumes of activity, which you would expect for different cities of such scales.\n\nSELECT\n  city,\n  YEAR(date) AS year,\n  SUM(CASE WHEN type = 'pedestrian' THEN 1 ELSE 0 END) AS pedestrian,\n  SUM(CASE WHEN type = 'vehicular' THEN 1 ELSE 0 END) AS vehicular\nFROM (\n\n    SELECT 'Long Beach' AS city, date, type\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, type\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, type\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) = 2014\n    AND type IN ('vehicular', 'pedestrian')\nGROUP BY\n    city, year\nORDER BY\n    city;\n\n\n3 records\n\n\ncity\nyear\npedestrian\nvehicular\n\n\n\n\nLong Beach\n2014\n2291\n21064\n\n\nMesa\n2014\n221\n27472\n\n\nSan Jose\n2014\n11176\n21485\n\n\n\n\n\nThis table summarizes stop counts for just the year 2014. There is one row per city and separate columns for pedestrian and vehicular stops. It makes it easy to compare both the overall scale of stops across cities and the relative size of pedestrian versus vehicular stops in the same year."
  },
  {
    "objectID": "project5.html#second-query",
    "href": "project5.html#second-query",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "Second Query",
    "text": "Second Query\nFor my next query, my goal is to analyze the racial mix of stops by type and city for Long Beach, Mesa, and San Jose, using the same 2014 to 2016 window as in the first query, to see how the racial distribution of stops differs across the three cities. This query gives the counts of stops for each combination of race, city, and stop type, and keeps only groups with at least 50 stops so that very rare combinations do not drive/meaningfully alter the patterns.\n\nSELECT\n  city,\n  type,\n  subject_race,\n  COUNT(*) AS n_stops\nFROM (\n\n    SELECT 'Long Beach' AS city, date, type, subject_race\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, type, subject_race\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, type, subject_race\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) &gt;= 2014\n    AND YEAR(date) &lt;= 2016\n    AND type IN ('vehicular', 'pedestrian')\n    AND subject_race IN (\n        'white',\n        'black',\n        'hispanic',\n        'asian/pacific islander',\n        'other/unknown'\n    )\nGROUP BY\n    city, type, subject_race\nHAVING\n    COUNT(*) &gt;= 50\nORDER BY\n    city, type, n_stops DESC;\n\nNow, I can use the output of the above query in my plot below:\n\nggplot(race_counts, aes(x = subject_race, y = n_stops, fill = type)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  facet_wrap(~ city) +\n  labs(\n    title = \"Racial Composition of Stops by City and Stop Type (2014–2016)\",\n    x = \"Subject race\",\n    y = \"Percent of stops\",\n    fill = \"Stop type\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nThis plot shows the racial makeup of police stops in Long Beach, Mesa, and San Jose between 2014 and 2016, separated into pedestrian and vehicular stops. Each bar represents one racial group and is scaled to 100 percent, so the height of each colored segment reflects the proportion of stops within that race category. Long Beach and San Jose show a visible mix of pedestrian and vehicular stops, while Mesa is almost exclusively vehicular in this dataset. The cities also differ in how heavily each racial group appears in the stop data, with San Jose showing the most variation across race and stop type compared to the other two cities. Overall, the chart highlights how both the racial distribution and the balance between pedestrian and vehicular stops vary from city to city.\n\nSELECT\n  city,\n  YEAR(date) AS year,\n  SUM(CASE WHEN subject_race = 'white' THEN 1 ELSE 0 END) AS white,\n  SUM(CASE WHEN subject_race = 'black' THEN 1 ELSE 0 END) AS black,\n  SUM(CASE WHEN subject_race = 'hispanic' THEN 1 ELSE 0 END) AS hispanic,\n  SUM(CASE WHEN subject_race = 'asian/pacific islander' THEN 1 ELSE 0 END) AS asian_pacific_islander,\n  SUM(CASE WHEN subject_race = 'other/unknown' THEN 1 ELSE 0 END) AS other_unknown\nFROM (\n\n    SELECT 'Long Beach' AS city, date, subject_race\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, subject_race\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, subject_race\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) = 2014\n    AND subject_race IN (\n        'white',\n        'black',\n        'hispanic',\n        'asian/pacific islander',\n        'other/unknown'\n    )\nGROUP BY\n    city, year\nORDER BY\n    city;\n\n\n3 records\n\n\n\n\n\n\n\n\n\n\n\ncity\nyear\nwhite\nblack\nhispanic\nasian_pacific_islander\nother_unknown\n\n\n\n\nLong Beach\n2014\n5885\n6228\n9710\n655\n0\n\n\nMesa\n2014\n19154\n2019\n5483\n381\n0\n\n\nSan Jose\n2014\n6811\n3603\n20172\n3526\n0\n\n\n\n\n\nThis 2014 table gives a simple view of how many stops involved each racial group in each city. It combines pedestrian and vehicular stops into a single snapshot to make the racial differences easy to compare across cities. For Long Beach, we see a relatively balanced distribution across white, Black, and Hispanic subjects, with smaller totals for Asian and Pacific Islander stops. Mesa shows fewer Black and Asian/Pacific Islander stops and a higher share of white stops, while San Jose has a noticeably larger number of Hispanic stops compared with the other cities."
  },
  {
    "objectID": "project5.html#third-query",
    "href": "project5.html#third-query",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "Third Query",
    "text": "Third Query\nFor my last query, I am going to be observing the search rates by race across the same three cities, Long Beach, Mesa, and San Jose, for 2014-2016. I am going to compare and try to understand whether searches happen at different rates for different groups.\n\nSELECT\n  city,\n  subject_race,\n  COUNT(*) AS total_stops,\n  SUM(CASE WHEN citation_issued &gt; 0 THEN 1 ELSE 0 END) AS citations,\n  SUM(CASE WHEN citation_issued &gt; 0 THEN 1 ELSE 0 END) / COUNT(*) AS citation_rate\nFROM (\n\n    SELECT 'Long Beach' AS city, date, subject_race, citation_issued\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, subject_race, citation_issued\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, subject_race, citation_issued\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) &gt;= 2014\n    AND YEAR(date) &lt;= 2016\n    AND subject_race IN (\n        'white',\n        'black',\n        'hispanic',\n        'asian/pacific islander',\n        'other/unknown'\n    )\nGROUP BY\n    city, subject_race\nHAVING\n    COUNT(*) &gt;= 50\nORDER BY\n    city, citation_rate DESC;\n\nNow, I can use the output of the above query in my plot below:\n\nggplot(citation_rates, aes(x = subject_race, y = citation_rate, fill = city)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = percent_format()) +\n  labs(\n    title = \"Citation Rates by Race Across Cities (2014–2016)\",\n    x = \"Subject race\",\n    y = \"Citation rate\",\n    fill = \"City\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nThis plot compares citation rates by race for Long Beach, Mesa, and San Jose between 2014 and 2016. For each racial group, the bars show the proportion of stops that resulted in a citation in each city. What we learn from this query is that Long Beach and Mesa both show citation rates at or near 100 percent for all groups, which suggests that these datasets mainly contain stops where a citation was issued. However, San Jose has much lower citation rates at around a quarter of stops, with only modest differences across racial groups, not enough of a difference to draw a firm conclusion. This highlights that the differences in citation rates are driven not only by variation across cities, but also by differences in how the underlying agencies record their stops. It is unlikely that San Jose is issuing significantly fewer citations in practice, but rather more likely that their data includes a broader range of stop outcomes than those reported by Long Beach and Mesa.\n\nSELECT\n  city,\n  SUM(CASE WHEN citation_issued &gt; 0 THEN 1 ELSE 0 END) AS citations,\n  COUNT(*) AS total_stops,\n  SUM(CASE WHEN citation_issued &gt; 0 THEN 1 ELSE 0 END) / COUNT(*) AS citation_rate\nFROM (\n\n    SELECT 'Long Beach' AS city, date, citation_issued\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, citation_issued\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, citation_issued\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) = 2014\nGROUP BY\n    city\nORDER BY\n    city;\n\n\n3 records\n\n\ncity\ncitations\ntotal_stops\ncitation_rate\n\n\n\n\nLong Beach\n25195\n25195\n1.0000\n\n\nMesa\n30288\n30288\n1.0000\n\n\nSan Jose\n9574\n37604\n0.2546\n\n\n\n\n\nThis 2014 table gives a compact summary of how many citations were issued in each city and what share of all stops resulted in a citation. It provides a single-year snapshot that makes it easy to compare how common citations were across the three cities.\n\nDBI::dbDisconnect(con_traffic)"
  },
  {
    "objectID": "project5.html#conclusion",
    "href": "project5.html#conclusion",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "Conclusion",
    "text": "Conclusion\nTo conclude, in this project I used SQL to pull and wrangle traffic stop data for Long Beach, Mesa, and San Jose from the Stanford Open Policing Project. I first combined the three city tables to track pedestrian and vehicular stops over time, then summarized the racial composition of stops by city and stop type, and finally compared citation rates across race and city. For each step I relied on SQL for all of the data wrangling and used R only to turn the summarized results into plots. I chose not to display the full output tables from the large queries that feed the plots, since those results span many years and produce long, paged tables that are hard to read and do not add much insight on the website and to readers. Instead, I created separate 2014 summary tables that give a clear, single year snapshot for each city, which makes the patterns in the three different queries easier to compare side by side.\nAcross the three analyses, a few patterns stood out. First, we saw that vehicular stops were far more common than pedestrian stops in all three cities, although the trends over time were different for each location. The distribution of racial categories in the stop records also varied across the cities, with San Jose showing a higher share of stops recorded as Hispanic compared with the other two datasets. In the citation analysis, Long Beach and Mesa showed higher citation rates across all groups, while San Jose showed lower rates overall. These differences point to variation in how each agency documents and processes its stops rather than anything about the groups themselves, and they highlight how comparing multiple cities side by side helps reveal differences in reporting and practice."
  },
  {
    "objectID": "project5.html#references",
    "href": "project5.html#references",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "References",
    "text": "References\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Project 4: Ethics - Data and Power - the COMPAS Algorithm",
    "section": "",
    "text": "This project examines COMPAS, a risk-assessment algorithm used by courts to estimate the likelihood that a defendant will reoffend. The system relies on historical criminal-justice data and claims to provide an objective measure of risk, in order to assist judges with bail or sentencing decisions. This is done with the hopes of limiting potential future crimes committed by these defendants.\nThe ethical dilemma appears when we look at how COMPAS performs across racial groups and how little transparency exists around its design. In 2016, ProPublica published a detailed analysis showing that the tool was more likely to falsely label black defendants as “high risk” while under-estimating risk for white defendants (ProPublica, 2016). Northpointe, which is the company behind COMPAS, disputed those findings by arguing that the model was properly calibrated and that bias appeared because different groups had different base rates of future re-arrest (Northpointe, 2016).\nThe tension between these two perspectives reveals the significance of this ethical issue, where an algorithm influences human freedom without open access, independent validation, or clear public understanding of its limitations and set up."
  },
  {
    "objectID": "project4.html#overview",
    "href": "project4.html#overview",
    "title": "Project 4: Ethics - Data and Power - the COMPAS Algorithm",
    "section": "",
    "text": "This project examines COMPAS, a risk-assessment algorithm used by courts to estimate the likelihood that a defendant will reoffend. The system relies on historical criminal-justice data and claims to provide an objective measure of risk, in order to assist judges with bail or sentencing decisions. This is done with the hopes of limiting potential future crimes committed by these defendants.\nThe ethical dilemma appears when we look at how COMPAS performs across racial groups and how little transparency exists around its design. In 2016, ProPublica published a detailed analysis showing that the tool was more likely to falsely label black defendants as “high risk” while under-estimating risk for white defendants (ProPublica, 2016). Northpointe, which is the company behind COMPAS, disputed those findings by arguing that the model was properly calibrated and that bias appeared because different groups had different base rates of future re-arrest (Northpointe, 2016).\nThe tension between these two perspectives reveals the significance of this ethical issue, where an algorithm influences human freedom without open access, independent validation, or clear public understanding of its limitations and set up."
  },
  {
    "objectID": "project4.html#what-is-the-permission-structure-for-using-the-data-and-was-it-followed",
    "href": "project4.html#what-is-the-permission-structure-for-using-the-data-and-was-it-followed",
    "title": "Project 4: Ethics - Data and Power - the COMPAS Algorithm",
    "section": "1. What is the permission structure for using the data, and was it followed?",
    "text": "1. What is the permission structure for using the data, and was it followed?\nCOMPAS was licensed to county and state justice departments, giving them permission to use local criminal records and demographic information (Northpointe, 2016). Judges, probation officers, and other officials were allowed to view the risk scores as one input in their decisions regarding the future of defendants. Legally, the data use followed official agreements.\nEthically, however, the people whose data built and informed the model never consented to those uses (ProPublica, 2016). Historical records of arrests and convictions were reused for predictive modeling without a transparent permission framework or community involvement. The permission structure prioritized institutional efficiency over individual rights or fairness."
  },
  {
    "objectID": "project4.html#who-was-measured-and-are-they-representative-of-the-population-to-whom-the-algorithm-is-applied",
    "href": "project4.html#who-was-measured-and-are-they-representative-of-the-population-to-whom-the-algorithm-is-applied",
    "title": "Project 4: Ethics - Data and Power - the COMPAS Algorithm",
    "section": "2. Who was measured, and are they representative of the population to whom the algorithm is applied?",
    "text": "2. Who was measured, and are they representative of the population to whom the algorithm is applied?\nAccording to ProPublica, the model was trained and validated on data from jurisdictions that already show racial disparities in arrests and sentencing (ProPublica, 2016). That means the algorithm ultimately learned patterns shaped by unequal policing. When applied to a broader population, it reproduced those disparities and ultimately showed that black defendants were twice as likely to be incorrectly labeled high risk (ProPublica, 2016).\nNorthpointe responded that the model’s predictions were “equally accurate” across groups when measured by calibration, meaning people with the same predicted risk reoffended at similar rates (Northpointe, 2016). Both statements can be true, but they highlight different fairness criteria. The group most measured, which was those already recorded in the justice system, is not necessarily representative of all communities affected by the algorithms decisions."
  },
  {
    "objectID": "project4.html#is-the-data-being-used-in-unintended-ways-relative-to-its-original-study",
    "href": "project4.html#is-the-data-being-used-in-unintended-ways-relative-to-its-original-study",
    "title": "Project 4: Ethics - Data and Power - the COMPAS Algorithm",
    "section": "3. Is the data being used in unintended ways relative to its original study?",
    "text": "3. Is the data being used in unintended ways relative to its original study?\nCOMPAS was marketed as a decision-support tool, not a strict decision-maker (Northpointe, 2016). However, once embedded in busy court systems, its scores began to influence outcomes directly. Judges and parole boards often treated the risk score as objective evidence, even though the methodology and weighting of variables were private and generally undisclosed (ProPublica, 2016).\nThis shift from support to decision substitute is an unintended use that shifts moral responsibility from humans to a an unclear algorithm. Without open auditing or clear guidance, the algorithm’s intended boundaries were crossed without much noise."
  },
  {
    "objectID": "project4.html#is-the-data-identifiable-or-sufficiently-anonymized",
    "href": "project4.html#is-the-data-identifiable-or-sufficiently-anonymized",
    "title": "Project 4: Ethics - Data and Power - the COMPAS Algorithm",
    "section": "4. Is the data identifiable or sufficiently anonymized?",
    "text": "4. Is the data identifiable or sufficiently anonymized?\nThe individual data used to train COMPAS was not publicly released, but remains highly identifiable inside the justice system (ProPublica, 2016). Each record corresponds to a specific defendant, linked to names, dates, and criminal histories. The algorithm’s secrecy means the public cannot evaluate whether identifiable variables such as race, neighborhood, or prior arrests were directly or indirectly included.\nBecause Northpointe treats its model as proprietary, outside researchers cannot confirm whether the data were anonymized at all (Northpointe, 2016), leaving privacy and fairness questions ultimately unresolved."
  },
  {
    "objectID": "project4.html#why-it-matters",
    "href": "project4.html#why-it-matters",
    "title": "Project 4: Ethics - Data and Power - the COMPAS Algorithm",
    "section": "Why it matters",
    "text": "Why it matters\nThe COMPAS controversy matters because it reveals how power and accountability can shift when decision-making is handed over to algorithmic systems. In the justice system, where every decision carries serious personal consequences, even small biases can have lasting effects on the defendant’s future. When a model like COMPAS produces a “risk score,” that number can shape how judges, prosecutors, and parole boards view a person’s future, which happens often without anyone fully understanding how the score was created.\n\nWho benefits: Private vendors like Northpointe gain steady revenue from government contracts, and public agencies can also claim that their decisions are based on “objective” data rather than personal judgment. The appearance of neutrality would be appealing since it suggests consistency and fairness, even when that impression may not match reality.\n\nWho is harmed: Defendants, especially black defendants, are the ones most affected when the algorithm’s predictions are inaccurate. A higher risk score can mean stricter bail terms, longer sentences, or reduced chances of parole. Since the model is closed to public review, those individuals have no real way to challenge or verify the system that labels them. This results in a frustrating situation where the defendants are unfairly fighting against a biased system, in this case being COMPAS.\nWhy it matters ethically: The use of predictive tools in criminal justice raises deep questions about consent and fairness. These systems often reinforce existing biases rather than correcting them. When technology reinforces social bias under the cover of data-driven objectivity, it shifts power towards institutions that already hold power, and away from the people most impacted by their decisions. True fairness requires visibility and questioning whether efficiency and profit are being prioritized over justice."
  },
  {
    "objectID": "project4.html#references",
    "href": "project4.html#references",
    "title": "Project 4: Ethics - Data and Power - the COMPAS Algorithm",
    "section": "References",
    "text": "References\nAngwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against Blacks. ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\nDieterich, W., Mendoza, C., & Brennan, T. (2016). COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity. Northpointe. https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf"
  },
  {
    "objectID": "olympics.html",
    "href": "olympics.html",
    "title": "Olympics Athletes and Medals",
    "section": "",
    "text": "Dataset page: TidyTuesday: Olympics 2024-08-06\nOriginal source: Sports-Reference.com Olympic History Project\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-06/olympics.csv')\n\nmedals_by_year &lt;- olympics |&gt;\n  filter(!is.na(medal)) |&gt;\n  group_by(year) |&gt;\n  summarise(n_medals = n())\n  \nggplot(medals_by_year, aes(x = year, y = n_medals)) +\n  geom_col(fill = \"blue\") +\n  labs(\n    title = \"Number of Recorded Medals by Year\",\n    x = \"Olympic Year\",\n    y = \"Medal Count\",\n    caption = \"Source: TidyTuesday 2024-08-06, Kaggle 120 Years of Olympic History\"\n  )"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Simulation Study",
    "section": "",
    "text": "Dataset page: R Datasets Package\nOriginal source: Henderson and Velleman (1981), Building Multiple Regression Models Interactively, Biometrics\n\n\n\nCode\nset.seed(47)\nlibrary(tidyverse)\nlibrary(purrr)\n\n\nI will compare MPG between manual and automatic cars in the build-in “mtcars” dataset. I will compute the observed difference (Manual - Automatic) and then simulate the null hypothesis, being “no relationship”, by permuting the transmission labels many times and recomputing the difference. I will report a one-sided p-value and show the null distribution, where the main variables we will be dealing with are “mpg” (numerical fuel economy) and “am” (categorical transmission: Automatics vs Manual). This will be interesting to analyze as MPG is a practical measure of efficiency that all drivers understand to be crucial for financial decisions when it comes to purchasing cars, so this simulation will help us understand if the relationship could arise by chance with no true relationship.\n\ncars &lt;- as_tibble(mtcars, rownames = \"model\") |&gt;\n  mutate(am = factor(am, c(0,1), labels = c(\"Automatic\", \"Manual\")))\ncars |&gt;\n  head(n=5)\n\n# A tibble: 5 × 12\n  model          mpg   cyl  disp    hp  drat    wt  qsec    vs am     gear  carb\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0 Manu…     4     4\n2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0 Manu…     4     4\n3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1 Manu…     4     1\n4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1 Auto…     3     1\n5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0 Auto…     3     2\n\n\n\nggplot(cars, aes(x = am, y = mpg, fill = am)) +\n  geom_boxplot(width = 0.55, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1) +\n  labs(\n    title = \"Fuel economy (MPG) by transmission\",\n    x = \"Transmission\", y = \"MPG\", fill = \"Transmission\"\n    ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis boxplot compares fuel economy, MPG, between cars with automatic and manual transmissions in the “mtcars” dataset. In the plot, every dot represents one car, and from the boxplot we can see that manual cars generally have higher MPG than automatic cars, with both a higher median and a wider spread of values. Thus, this suggests that manual transmissions may be more fuel-efficent on average in this dataset.\nTo visualize the original data:\n\ncars |&gt;\n  group_by(am) |&gt;\n  summarise(\n    n = n(),\n    mean_mpg = mean(mpg),\n    median_mpg = median(mpg)\n  )\n\n# A tibble: 2 × 4\n  am            n mean_mpg median_mpg\n  &lt;fct&gt;     &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Automatic    19     17.1       17.3\n2 Manual       13     24.4       22.8\n\n\nNow, for the simulation part:\n\nmean_diff &lt;- function(data){\n  manual_mpg &lt;- mean(data$mpg[data$am == \"Manual\"])\n  auto_mpg   &lt;- mean(data$mpg[data$am == \"Automatic\"])\n  return(manual_mpg - auto_mpg)\n}\n\nperm_once &lt;- function(data){\n  data_perm &lt;- data\n  data_perm$am &lt;- sample(data_perm$am)\n  mean_diff(data_perm)\n}\n\nObserved difference in means:\n\nobs &lt;- mean_diff(cars)\nobs\n\n[1] 7.244939\n\n\nNull distribution under “no relationship” and one-side p-value:\n\nB &lt;- 5000\nnull_vals &lt;- map_dbl(1:B, function(i) perm_once(cars))\np_value &lt;- mean(null_vals &gt;= obs)\np_value_formatted &lt;- format(p_value, scientific = FALSE, digits = 4)\n\ntibble(\n  observed_diff = obs,\n  p_value_one_sided = p_value_formatted,\n  permutations = B\n)\n\n# A tibble: 1 × 3\n  observed_diff p_value_one_sided permutations\n          &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n1          7.24 0                         5000\n\n\nThe p_value_one_sided is actually equal to 0.0004, but R rounds it down to 0.\nDescription of the simulation:\nThe simulation works by repeatedly shuffling the transmission labels to represent an environment where there is no true relationship between transmission type and fuel economy (the null hypothesis). - The function mean_diff() calculates the difference in average MPG between manual and automatic cars for any given dataset. - The function perm_once() performs one permutation by randomly reassigning transmission labels and then calling mean_diff() to get a new difference value. - I used map_dbl() to repeat this process 5,000 times, because it provides a fast way to apply the same function many times and store the numeric results in a single vector. This created a simulated null distribution of MPG differences that we can compare the observed difference against to determine how different it is the under the null model.\nPlotting the null distribution:\n\ntibble(null = null_vals) |&gt;\n  ggplot(aes(x = null)) +\n  geom_histogram(bins = 40, alpha = 0.85) +\n  geom_vline(xintercept = obs, linetype = \"dashed\") +\n  labs(\n    title = \"Permutation null for MPG difference (Manual − Automatic)\",\n    x = \"Null statistic (Manual − Automatic mean MPG)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis histogram shows the null distribution of differences in mean MPG between manual and automatic cars, created by randomly shuffling the transmission labels 5,000 times. The x-axis represents the simulated differences under the null hypothesis of no relationship between transmission and MPG, with the y-axis showing how often each difference occurred. The dashed vertical line marks the observed difference from the real data, which was about 7.24 MPG. This observed value lies far to the right of all simulated values, and thus suggests that such a large difference would be extremely unlikely if transmission had no effect on fuel economy.\nTo conclude:\nIn this analysis, I conducted a permutation test to determine whether cars with manual transmission have higher fuel economy, MPG, than cars with automatic transmission in the “mtcars” dataset. I computed the observed difference in mean MPG and then simulated the null hypothesis by repeatedly shuffling transmission labels and recalculating the mean difference 5,000 times. The resulting null distribution was centered around 0, while the observed difference (7.24 MPG) was far outside this range, giving a p-value of 0.0004, which is less than 0.01. This indicated that it is highly unlikely the difference in MPG occurred by chance, providing very strong evidence against the null model, and ultimately that manual cars are more fuel-efficient on average."
  },
  {
    "objectID": "kidzbop.html",
    "href": "kidzbop.html",
    "title": "Kidz Bop Censored Lyrics",
    "section": "",
    "text": "Dataset page: The Pudding – Kidz Bop Data\nOriginal source: Kidz Bop Censored Lyrics Dataset (Hehmeyer et al., The Pudding)\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)\n\n\n\nkb &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/kidz-bop/KB_censored-lyrics.csv\")\n\ncategory_counts &lt;- kb |&gt;\n  count(category) |&gt;\n  head(10)\n\ntop8 &lt;- head(category_counts, 8) \n\nggplot(data = top8, aes(x = reorder(category, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"What Kidz Bop censors most\",\n    subtitle = \"Top categories in the censored-lyrics dataset\",\n    x = \"Category\",\n    y = \"Censored instances\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot helps to show which types of content are most frequently censored in Kidz Bop songs. From the counts, we can see that profanity and sexual references make up the largest portion of censored lyrics, followed closely by alcohol and drug references. Categories like violence, identity, and other themes appear much less often.\nThese patterns reveal that Kidz Bop’s censorship choices focus mainly on removing language that could be considered inappropriate or too mature for a younger audience, while other categories are less frequently flagged. This highlights how producers are likely prioritizing a “family-friendly” tone in their songs.\n\nkb|&gt;\n  mutate(my_word=str_extract_all(ogLyric, \"(?&lt;=\\\\bmy\\\\s)\\\\w+\")) |&gt;\n  select(my_word) |&gt;\n  unnest(my_word) |&gt;\n  count(my_word, sort = TRUE) |&gt;\n  head(10)\n\n# A tibble: 10 × 2\n   my_word       n\n   &lt;chr&gt;     &lt;int&gt;\n 1 nights       18\n 2 woman        16\n 3 God          11\n 4 body          8\n 5 cigarette     8\n 6 hand          8\n 7 neck          7\n 8 chest         5\n 9 ex            5\n10 lips          5\n\n\nThis table shows which words most often follow the word “my” in the original lyrics of the Kidz Bop songs. We can see that the top three words that follow “my” are nights, woman, and God, highlighting the personal and relational themes in censored lyrics.\n\nkb |&gt;\n  mutate(first_three_words = str_extract(ogLyric, \"^(?:\\\\S+\\\\s+){0,2}\\\\S+\")) |&gt;  \n  count(first_three_words, sort = TRUE) |&gt;\n  head(10)\n\n# A tibble: 10 × 2\n   first_three_words       n\n   &lt;chr&gt;               &lt;int&gt;\n 1 Blow a kiss,           36\n 2 Be my woman,           16\n 3 The Bowery, whiskey    10\n 4 And I don't             8\n 5 I'm too hot             8\n 6 Kissed her on           8\n 7 Ain't another woman     7\n 8 And I got               7\n 9 Can't drink without     7\n10 I want you              7\n\n\nThis table shows the most common three-word openings found in the original lyrics of Kidz Bop songs. We can see that many of the top phrases like “Blow a kiss”, “Be my woman”, and “Kissed her on” start with actions and expressions of relationships. These repeated openings show that censored lyrics often begin with personal and romantic themes that would likely need to be toned down for the younger audience of Kidz Bop.\n\nkb_lower &lt;- kb |&gt;\n  filter(!is.na(year), !is.na(ogLyric))|&gt;\n  mutate(og_lower = str_to_lower(ogLyric))\n\nreg_expression &lt;- \"\\\\b(love|lover|kiss|kissing|baby|girlfriend|boyfriend|girl|boy|wife|husband|ex|partner)\\\\b\"\n\nby_year &lt;- kb_lower |&gt;\n  mutate(has_relationship = str_detect(og_lower, reg_expression)) |&gt;\n  group_by(year) |&gt;\n  summarise(total = n(), relationship = sum(has_relationship, na.rm = TRUE), pct = relationship / total) |&gt;\n  filter(total &gt;= 20) |&gt; # helps to stabilize the percentages\n  ungroup()\n\nggplot(by_year, aes(x = year, y = pct)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Lyrics mentioning relationships over time\",\n    subtitle = \"Share of original lyrics with partner or family terms (years with 20+ rows)\",\n    x = \"Year\",\n    y = \"Percent of lyrics mentioning relationships\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plots shows how often relationship and love-related words such as love, kiss, baby, girlfriend, boyfriend, etc. appear in the original Kidz Bop lyrics over time. Although the percentages remain small overall, there are clear ups and downs across the years. At peaks around 2014-2015, the data suggests that more popular songs from those years contained romantic and affectionate language that Kidz Bop chose to censor and modify. On the other hand, lower points indicate years when songs with relationship themes were a little bit less common. Overall, we can see how the romantic tone of pop music can change over time and how Kidz Bop’s censorship accommodates those changing tones."
  }
]