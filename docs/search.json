[
  {
    "objectID": "premierleague.html",
    "href": "premierleague.html",
    "title": "Premier League 2021–22",
    "section": "",
    "text": "Dataset page: TidyTuesday: Premier League Match Data 2023-04-04\nOriginal source: Official Premier League Statistics\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-04-04/soccer21-22.csv')\n\nteam_goals &lt;- soccer |&gt;\n  group_by(HomeTeam) |&gt;\n  summarise(avg_goals = mean(FTHG, na.rm = TRUE))\n\nggplot(team_goals, aes(x = HomeTeam, y = avg_goals)) +\n  geom_col(fill = \"blue\") +\n  labs(\n    title = \"Average Home Goals per Match by Team (2021–22)\",\n    x = \"Team\",\n    y = \"Average Goals\",\n    caption = \"Source: TidyTuesday 2023-04-04, Premier League 2021–22\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I grew up in Irvine, California, and have two sisters and a pet dog named Luna. Here are some pictures of us:\nMy favorite sport is soccer, and I am a member of the Pomona-Pitzer Men’s soccer team here on campus. Some action shots:\nOne of my favorite things to do in my free time is travel, whether it be to new countries or to local hiking spots in the area. Some of my adventures:\nThanks for stopping by!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guy Fuchs",
    "section": "",
    "text": "Hi there, my name is Guy Fuchs and I am a junior at Pomona College studying computer science and physics. I’ve always been interested in data science as well, so I’ve created this website to showcase some of my mini-projects I’ve worked on recently!"
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "",
    "text": "For this project I want to explore how police stop patterns differ across several cities included in the Stanford Open Policing Project. I plan to pull data from three separate city-level tables (Long Beach in CA, Mesa in AZ, and San Jose in CA) and use SQL to compare how many pedestrian and vehicular stops occur over time, how the racial mix of the people stopped varies, and how often citations are issued across race. My plan is to combine the data sets, summarize and analyze them using different SQL queries, and then visualize the results with plots that will highlight differences across cities and stop types. My main goal is to see whether the data suggests meaningful variations in who gets stopped or cited, and whether those patterns look similar or different across the three cities.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DBI)\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)"
  },
  {
    "objectID": "project5.html#introduction",
    "href": "project5.html#introduction",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "",
    "text": "For this project I want to explore how police stop patterns differ across several cities included in the Stanford Open Policing Project. I plan to pull data from three separate city-level tables (Long Beach in CA, Mesa in AZ, and San Jose in CA) and use SQL to compare how many pedestrian and vehicular stops occur over time, how the racial mix of the people stopped varies, and how often citations are issued across race. My plan is to combine the data sets, summarize and analyze them using different SQL queries, and then visualize the results with plots that will highlight differences across cities and stop types. My main goal is to see whether the data suggests meaningful variations in who gets stopped or cited, and whether those patterns look similar or different across the three cities.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DBI)\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)"
  },
  {
    "objectID": "project5.html#first-query",
    "href": "project5.html#first-query",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "First Query",
    "text": "First Query\nFor my first query, my goal is to combine three city tables and keep only vehicular/pedestrian, extract the year, and then count stops by city-year-type. I restrict the year span to be the years 2014 through 2016, which is the period that Mesa, Long Beach, and San Jose all report data in the SOPP database.\n\nSELECT\n  city,\n  YEAR(date) AS year,\n  type,\n  COUNT(*) AS n_stops\nFROM (\n\n    SELECT 'Mesa' AS city, date, type\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'Long Beach' AS city, date, type\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, type\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) &gt;= 2014\n    AND YEAR(date) &lt;= 2016\n    AND type IN ('vehicular', 'pedestrian')\nGROUP BY\n    city, year, type\nORDER BY\n    city, year, type;\n\nNow, I can use the output of the above query in my plot below:\n\nggplot(stops_by_year_type |&gt;\n         mutate(year = as.integer(year), n_stops = as.numeric(n_stops)), \n       aes(x = year, y = n_stops, color = type)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ city) +\n  scale_x_continuous(breaks = 2014:2016) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    title = \"Traffic and Pedestrian Stops Over Time\",\n    x = \"Year\",\n    y = \"Number of stops\",\n    color = \"Stop type\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n  axis.text.x = element_text(angle = 45, hjust = 1),\n  panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nThis plot shows how the number of traffic stops changes over time for Long Beach, Mesa, and San Jose. I focused on pedestrian and vehicular stops between 2014 and 2016 so that all three cities share a common window of data, and we can see that vehicular stops dominate the totals in every city, and the overall levels differ quite a bit across the three locations. The trends themselves are also different, which gives the idea that each department operates under its own patterns and volumes of activity, which you would expect for different cities of such scales.\n\nSELECT\n  city,\n  YEAR(date) AS year,\n  SUM(CASE WHEN type = 'pedestrian' THEN 1 ELSE 0 END) AS pedestrian,\n  SUM(CASE WHEN type = 'vehicular' THEN 1 ELSE 0 END) AS vehicular\nFROM (\n\n    SELECT 'Long Beach' AS city, date, type\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, type\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, type\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) = 2014\n    AND type IN ('vehicular', 'pedestrian')\nGROUP BY\n    city, year\nORDER BY\n    city;\n\n\n3 records\n\n\ncity\nyear\npedestrian\nvehicular\n\n\n\n\nLong Beach\n2014\n2291\n21064\n\n\nMesa\n2014\n221\n27472\n\n\nSan Jose\n2014\n11176\n21485\n\n\n\n\n\nThis table summarizes stop counts for just the year 2014. There is one row per city and separate columns for pedestrian and vehicular stops. It makes it easy to compare both the overall scale of stops across cities and the relative size of pedestrian versus vehicular stops in the same year."
  },
  {
    "objectID": "project5.html#second-query",
    "href": "project5.html#second-query",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "Second Query",
    "text": "Second Query\nFor my next query, my goal is to analyze the racial mix of stops by type and city for Long Beach, Mesa, and San Jose, using the same 2014 to 2016 window as in the first query, to see how the racial distribution of stops differs across the three cities. This query gives the counts of stops for each combination of race, city, and stop type, and keeps only groups with at least 50 stops so that very rare combinations do not drive/meaningfully alter the patterns.\nA subtle point in this query is how the “HAVING COUNT(*) &gt;= 50 condition works. Since the data is grouped by city, stop type, and subject race, the HAVING clause applies to each city-type-race combination. That means a pedestrian bar for a given race will be dropped if there are fewer than 50 stops for that race in that city, even if there are many vehicular stops for the same race there. I decided that was acceptable here, because the groups that get removed are very small and would give noisy percentages, but is is important to keep in mind that the 50 stop filter is acting at the city-type-race level rather than only city-race.\n\nSELECT\n  city,\n  type,\n  subject_race,\n  COUNT(*) AS n_stops\nFROM (\n\n    SELECT 'Long Beach' AS city, date, type, subject_race\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, type, subject_race\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, type, subject_race\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) &gt;= 2014\n    AND YEAR(date) &lt;= 2016\n    AND type IN ('vehicular', 'pedestrian')\n    AND subject_race IN (\n        'white',\n        'black',\n        'hispanic',\n        'asian/pacific islander',\n        'other/unknown'\n    )\nGROUP BY\n    city, type, subject_race\nHAVING\n    COUNT(*) &gt;= 50\nORDER BY\n    city, type, n_stops DESC;\n\nNow, I can use the output of the above query in my plot below:\n\nggplot(race_counts, aes(x = subject_race, y = n_stops, fill = type)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  facet_wrap(~ city) +\n  labs(\n    title = \"Racial Composition of Stops by City and Stop Type (2014–2016)\",\n    x = \"Subject race\",\n    y = \"Percent of stops\",\n    fill = \"Stop type\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nThis plot shows the racial makeup of police stops in Long Beach, Mesa, and San Jose between 2014 and 2016, separated into pedestrian and vehicular stops. Each bar represents one racial group and is scaled to 100 percent, so the height of each colored segment reflects the proportion of stops within that race category. Also, very small city-type-race combinations with fewer than 50 stops are excluded so that each bar segment represents a reasonably sized group. Long Beach and San Jose show a visible mix of pedestrian and vehicular stops, while Mesa is almost exclusively vehicular in this dataset. The cities also differ in how heavily each racial group appears in the stop data, with San Jose showing the most variation across race and stop type compared to the other two cities. Overall, the chart highlights how both the racial distribution and the balance between pedestrian and vehicular stops vary from city to city.\n\nSELECT\n  city,\n  YEAR(date) AS year,\n  SUM(CASE WHEN subject_race = 'white' THEN 1 ELSE 0 END) AS white,\n  SUM(CASE WHEN subject_race = 'black' THEN 1 ELSE 0 END) AS black,\n  SUM(CASE WHEN subject_race = 'hispanic' THEN 1 ELSE 0 END) AS hispanic,\n  SUM(CASE WHEN subject_race = 'asian/pacific islander' THEN 1 ELSE 0 END) AS asian_pacific_islander,\n  SUM(CASE WHEN subject_race = 'other/unknown' THEN 1 ELSE 0 END) AS other_unknown\nFROM (\n\n    SELECT 'Long Beach' AS city, date, subject_race\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, subject_race\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, subject_race\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) = 2014\n    AND subject_race IN (\n        'white',\n        'black',\n        'hispanic',\n        'asian/pacific islander',\n        'other/unknown'\n    )\nGROUP BY\n    city, year\nORDER BY\n    city;\n\n\n3 records\n\n\n\n\n\n\n\n\n\n\n\ncity\nyear\nwhite\nblack\nhispanic\nasian_pacific_islander\nother_unknown\n\n\n\n\nLong Beach\n2014\n5885\n6228\n9710\n655\n0\n\n\nMesa\n2014\n19154\n2019\n5483\n381\n0\n\n\nSan Jose\n2014\n6811\n3603\n20172\n3526\n0\n\n\n\n\n\nThis 2014 table gives a simple view of how many stops involved each racial group in each city. It combines pedestrian and vehicular stops into a single snapshot to make the racial differences easy to compare across cities. For Long Beach, we see a relatively balanced distribution across white, Black, and Hispanic subjects, with smaller totals for Asian and Pacific Islander stops. Mesa shows fewer Black and Asian/Pacific Islander stops and a higher share of white stops, while San Jose has a noticeably larger number of Hispanic stops compared with the other cities."
  },
  {
    "objectID": "project5.html#third-query",
    "href": "project5.html#third-query",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "Third Query",
    "text": "Third Query\nFor my last query, I am going to be observing the citation rates by race across the same three cities, Long Beach, Mesa, and San Jose, for 2014-2016. I am going to compare and try to understand whether searches happen at different rates for different groups.\n\nSELECT\n  city,\n  subject_race,\n  COUNT(*) AS total_stops,\n  SUM(CASE WHEN citation_issued &gt; 0 THEN 1 ELSE 0 END) AS citations,\n  SUM(CASE WHEN citation_issued &gt; 0 THEN 1 ELSE 0 END) / COUNT(*) AS citation_rate\nFROM (\n\n    SELECT 'Long Beach' AS city, date, subject_race, citation_issued\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, subject_race, citation_issued\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, subject_race, citation_issued\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND citation_issued IS NOT NULL\n    AND YEAR(date) &gt;= 2014\n    AND YEAR(date) &lt;= 2016\n    AND subject_race IN (\n        'white',\n        'black',\n        'hispanic',\n        'asian/pacific islander',\n        'other/unknown'\n    )\nGROUP BY\n    city, subject_race\nHAVING\n    COUNT(*) &gt;= 50\nORDER BY\n    city, citation_rate DESC;\n\nNow, I can use the output of the above query in my plot below:\n\nggplot(citation_rates, aes(x = subject_race, y = citation_rate, fill = city)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = percent_format()) +\n  labs(\n    title = \"Citation Rates by Race Across Cities (2014–2016)\",\n    x = \"Subject race\",\n    y = \"Citation rate\",\n    fill = \"City\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nThis plot compares citation rates by race for Long Beach, Mesa, and San Jose between 2014 and 2016. For each racial group, the bars show the proportion of stops that resulted in a citation in each city. What we learn from this query is that Long Beach and Mesa both show citation rates at or near 100 percent for all groups, which suggests that these datasets mainly contain stops where a citation was issued. To take it one step further, we can potentially conclude that the datasets for Long Beach and Mesa may only include stops that led to a citation, or that non-citation outcomes were not consistently recorded or not recorded at all.\nHowever, San Jose has much lower citation rates at around a quarter of stops, with only modest differences across racial groups, not enough of a difference to draw a firm conclusion. This highlights that the differences in citation rates are driven not only by variation across cities, but also by differences in how the underlying agencies record their stops. It is unlikely that San Jose is issuing significantly fewer citations in practice, but rather more likely that their data includes a broader range of stop outcomes than those reported by Long Beach and Mesa.\nMoreover,\n\nSELECT\n  city,\n  SUM(CASE WHEN citation_issued &gt; 0 THEN 1 ELSE 0 END) AS citations,\n  COUNT(*) AS total_stops,\n  SUM(CASE WHEN citation_issued &gt; 0 THEN 1 ELSE 0 END) / COUNT(*) AS citation_rate\nFROM (\n\n    SELECT 'Long Beach' AS city, date, citation_issued\n    FROM ca_long_beach_2020_04_01\n\n    UNION ALL\n\n    SELECT 'Mesa' AS city, date, citation_issued\n    FROM az_mesa_2023_01_26\n\n    UNION ALL\n\n    SELECT 'San Jose' AS city, date, citation_issued\n    FROM ca_san_jose_2020_04_01\n\n) AS all_stops\nWHERE\n    date IS NOT NULL\n    AND YEAR(date) = 2014\nGROUP BY\n    city\nORDER BY\n    city;\n\n\n3 records\n\n\ncity\ncitations\ntotal_stops\ncitation_rate\n\n\n\n\nLong Beach\n25195\n25195\n1.0000\n\n\nMesa\n30288\n30288\n1.0000\n\n\nSan Jose\n9574\n37604\n0.2546\n\n\n\n\n\nThis 2014 table gives a compact summary of how many citations were issued in each city and what share of all stops resulted in a citation. It provides a single-year snapshot that makes it easy to compare how common citations were across the three cities.\n\nDBI::dbDisconnect(con_traffic)"
  },
  {
    "objectID": "project5.html#conclusion",
    "href": "project5.html#conclusion",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "Conclusion",
    "text": "Conclusion\nTo conclude, in this project I used SQL to pull and wrangle traffic stop data for Long Beach, Mesa, and San Jose from the Stanford Open Policing Project. I first combined the three city tables to track pedestrian and vehicular stops over time, then summarized the racial composition of stops by city and stop type, and finally compared citation rates across race and city. For each step I relied on SQL for all of the data wrangling and used R only to turn the summarized results into plots. I chose not to display the full output tables from the large queries that feed the plots, since those results span many years and produce long, paged tables that are hard to read and do not add much insight on the website and to readers. Instead, I created separate 2014 summary tables that give a clear, single year snapshot for each city, which makes the patterns in the three different queries easier to compare side by side.\nAcross the three analyses, a few patterns stood out. First, we saw that vehicular stops were far more common than pedestrian stops in all three cities, although the trends over time were different for each location. The distribution of racial categories in the stop records also varied across the cities, with San Jose showing a higher share of stops recorded as Hispanic compared with the other two datasets. In the citation analysis, Long Beach and Mesa showed higher citation rates across all groups, while San Jose showed lower rates overall. These differences point to variation in how each agency documents and processes its stops rather than anything about the groups themselves, and they highlight how comparing multiple cities side by side helps reveal differences in reporting and practice. The citation results reinforced this idea by showing how agency level recording choices can shape the patterns we see, since Long Beach and Mesa appear to record mainly citation producing stops, while San Jose includes a broader set of outcomes."
  },
  {
    "objectID": "project5.html#references",
    "href": "project5.html#references",
    "title": "SQL Analysis of Traffic Stop Data",
    "section": "References",
    "text": "References\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Ethics - Data and Power - the COMPAS Algorithm",
    "section": "",
    "text": "COMPAS is a risk-assessment algorithm used by some US courts to estimate the likelihood that a defendant will reoffend. The tool was developed by Northpointe and is built on proprietary criminal-justice data that the public cannot access or evaluate (Dieterich et al., 2016). Courts use the resulting score as one factor in bail, sentencing, and supervision decisions. This is done with the hopes of limiting potential future crimes committed by these defendants.\nMore public scrutiny intensified after Angwin et al. (2016) analyzed COMPAS scores from Broward County and reported that Black defendants were more likely to be labeled high-risk but did not actually reoffend, while white defendants were more likely to be labeled low-risk but later did reoffend. Northpointe argued that the tool was well-calibrated overall and that differences arose from underlying disparities in the criminal-justice system.\nThese competing perspectives highlight how some algorithmic tools influence decisions about liberty and freedom without clear public understanding of how those tools operate."
  },
  {
    "objectID": "project4.html#overview",
    "href": "project4.html#overview",
    "title": "Ethics - Data and Power - the COMPAS Algorithm",
    "section": "",
    "text": "COMPAS is a risk-assessment algorithm used by some US courts to estimate the likelihood that a defendant will reoffend. The tool was developed by Northpointe and is built on proprietary criminal-justice data that the public cannot access or evaluate (Dieterich et al., 2016). Courts use the resulting score as one factor in bail, sentencing, and supervision decisions. This is done with the hopes of limiting potential future crimes committed by these defendants.\nMore public scrutiny intensified after Angwin et al. (2016) analyzed COMPAS scores from Broward County and reported that Black defendants were more likely to be labeled high-risk but did not actually reoffend, while white defendants were more likely to be labeled low-risk but later did reoffend. Northpointe argued that the tool was well-calibrated overall and that differences arose from underlying disparities in the criminal-justice system.\nThese competing perspectives highlight how some algorithmic tools influence decisions about liberty and freedom without clear public understanding of how those tools operate."
  },
  {
    "objectID": "project4.html#what-is-the-permission-structure-for-using-the-data-and-was-it-followed",
    "href": "project4.html#what-is-the-permission-structure-for-using-the-data-and-was-it-followed",
    "title": "Ethics - Data and Power - the COMPAS Algorithm",
    "section": "1. What is the permission structure for using the data, and was it followed?",
    "text": "1. What is the permission structure for using the data, and was it followed?\nNorthpointe licenses COMPAS to courts, which are permitted to enter defendants’ criminal histories, interview responses, and demographic information into the software (Dieterich et al., 2016). Because criminal-justice data are collected under legal mandate, courts have institutional permission to use this information.\nAngwin et al. (2016) point out that individuals do not provide explicit consent for their personal records to be used in proprietary algorithms. So, we can conclude that the absence of consent is structural as defendants cannot opt out of having their information included. The permission structure reflects institutional needs rather than voluntary participation."
  },
  {
    "objectID": "project4.html#who-was-measured-and-are-they-representative-of-the-population-to-whom-the-algorithm-is-applied",
    "href": "project4.html#who-was-measured-and-are-they-representative-of-the-population-to-whom-the-algorithm-is-applied",
    "title": "Ethics - Data and Power - the COMPAS Algorithm",
    "section": "2. Who was measured, and are they representative of the population to whom the algorithm is applied?",
    "text": "2. Who was measured, and are they representative of the population to whom the algorithm is applied?\nNorthpointe does not disclose the exact dataset used to train COMPAS. They state only that it reflects historical patterns from jurisdictions that purchased the tool (Dieterich et al., 2016).\nThe dataset analyzed by Angwin et al. (2016) includes people arrested in Broward County. This group reflects local policing and arrest patterns, which differ across communities. When COMPAS is applied broadly, many defendants scored by the algorithm may not resemble the individuals who informed the model.\nThis matters because the model inherits patterns from its training data, so overrepresentaiton of certain groups during data collection can lead to unequal outcomes during deployment."
  },
  {
    "objectID": "project4.html#is-the-data-being-used-in-unintended-ways-relative-to-its-original-study",
    "href": "project4.html#is-the-data-being-used-in-unintended-ways-relative-to-its-original-study",
    "title": "Ethics - Data and Power - the COMPAS Algorithm",
    "section": "3. Is the data being used in unintended ways relative to its original study?",
    "text": "3. Is the data being used in unintended ways relative to its original study?\nNorthpointe describes COMPAS as a decision-support tool rather than a replacement for judicial judgment (Dieterich et al., 2016). Yet Angwin et al. (2016) show that courts often treat the score as an authoritative measure of risk. In fast-paced environments, the score can become a stand-in for a more nuanced evaluation. This is an issue as judges and parole boards often treated the risk score as objective evidence.\nThis shift from advisory tool to decision substitute is an unintended use that shifts moral responsibility from humans to a an unclear algorithm. Because the model is proprietary, defendants cannot examine or contest the score that influences their case, which raises concerns about accountability and fairness."
  },
  {
    "objectID": "project4.html#is-the-data-identifiable-or-sufficiently-anonymized",
    "href": "project4.html#is-the-data-identifiable-or-sufficiently-anonymized",
    "title": "Ethics - Data and Power - the COMPAS Algorithm",
    "section": "4. Is the data identifiable or sufficiently anonymized?",
    "text": "4. Is the data identifiable or sufficiently anonymized?\nCOMPAS training data are private and undisclosed, so anonymization is not the central question. The ethical concern is the opacity of the model as neither the training data nor the variable weights can be independently reviewed.\nThe dataset used by Angwin et al. (2016) is different, where these records were obtained through public records requests and include defendants’ names, charges, and outcomes. Criminal justice records are by nature identifiable, and this data was not anonymized. The core privacy issue, therefore, is not anonymity but transparency. Without access to the model, the public cannot evaluate whether sensitive variables, or substitutes for those variables, were used."
  },
  {
    "objectID": "project4.html#why-it-matters",
    "href": "project4.html#why-it-matters",
    "title": "Ethics - Data and Power - the COMPAS Algorithm",
    "section": "Why it matters",
    "text": "Why it matters\nAlgorithmic tools like COMPAS shift power in the criminal-justice process. Courts rely on a proprietary score that defendants cannot see, explain, or meaningfully challenge.This controversy matters because it reveals how power and accountability can shift when decision-making is handed over to algorithmic systems. In the justice system, where every decision carries serious personal consequences, even small biases can have lasting effects on the defendant’s future. When a model like COMPAS produces a “risk score,” that number can shape how judges, prosecutors, and parole boards view a person’s future, which happens often without anyone fully understanding how the score was created.\n\nWho benefits: Private vendors like Northpointe gain steady revenue from government contracts, and public agencies can also claim that their decisions are based on “objective” data rather than personal judgment. Courts would also gain a more streamlined workflow, and the appearance of neutrality would be appealing since it suggests consistency and fairness, even when that impression may not match reality.\nWho is harmed: Angwin et al. (2016) showed that defendants, especially Black defendants, are the ones most affected when the algorithm’s predictions are inaccurate. A higher risk score can mean stricter bail terms, longer sentences, or reduced chances of parole. Since the model is closed to public review, those individuals have no real way to challenge or verify the system that labels them. This results in a frustrating situation where the defendants are unfairly fighting against a biased system, in this case being COMPAS.\nWhy it matters ethically: The use of predictive tools in criminal justice raises deep questions about transparency, fairness, and the distribution of power. These systems often reinforce existing biases rather than correcting them. When technology reinforces social bias under the cover of data-driven objectivity, it shifts power towards institutions that already hold power, and away from the people most impacted by their decisions. True fairness requires visibility and questioning whether efficiency and profit are being prioritized over justice."
  },
  {
    "objectID": "project4.html#references",
    "href": "project4.html#references",
    "title": "Ethics - Data and Power - the COMPAS Algorithm",
    "section": "References",
    "text": "References\nAngwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against Blacks. ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\nDieterich, W., Mendoza, C., & Brennan, T. (2016). COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity. Northpointe. https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf"
  },
  {
    "objectID": "olympics.html",
    "href": "olympics.html",
    "title": "Olympics Athletes and Medals",
    "section": "",
    "text": "Dataset page: TidyTuesday: Olympics 2024-08-06\nOriginal source: Sports-Reference - Olympic History Project\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-06/olympics.csv')\n\nmedals_by_year &lt;- olympics |&gt;\n  filter(!is.na(medal)) |&gt;\n  group_by(year) |&gt;\n  summarise(n_medals = n())\n  \nggplot(medals_by_year, aes(x = year, y = n_medals)) +\n  geom_col(fill = \"blue\") +\n  labs(\n    title = \"Number of Recorded Medals by Year\",\n    x = \"Olympic Year\",\n    y = \"Medal Count\",\n    caption = \"Source: TidyTuesday 2024-08-06, Kaggle 120 Years of Olympic History\"\n  )"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Simulation Study",
    "section": "",
    "text": "Dataset page: R Datasets Package\nOriginal source: The data was originally published in the 1974 Motor Trend US Magazine and later referenced in Henderson and Velleman (1981), Building Multiple Regression Models Interactively, Biometrics.\n\n\n\nCode\nset.seed(47)\nlibrary(tidyverse)\nlibrary(purrr)\n\n\n\nOutline\nI will compare MPG between manual and automatic cars in the built in mtcars dataset. The data comes from the 1974 Motor Trend US magazine, so I treat the 32 cars in the dataset as a sample from a broader population of early 1970s cars similar to those reviewed by Motor Trend, not from modern cars, and so any inferences and conclusions made will be about that population, not to modern vehicles. This will be interesting to analyze as MPG is a practical measure of efficiency, so this simulation will help us understand if the relationship could arise by chance with no true relationship.\nMy goal is to test whether manual transmissions in that 1970s era population tend to have higher fuel economy than automatics. To make that precise, I will use the following hypotheses:\n\nNull hypothesis (H₀): In the population of early 1970s cars, manual transmissions do not have higher mean MPG than automatics. Under H₀, the true difference in mean MPG (Manual − Automatic) is zero or negative.\n\nAlternative hypothesis (H₁): In that same population, manual transmissions have higher mean MPG than automatics, so the true difference in mean MPG (Manual − Automatic) is positive.\n\nThis test will compare two varaibles from the dataset, the first being mpg, a numerical value representing miles per gallon for each car. The second is am, indicating transmission type. In the raw dataset, 0 means automatic and 1 means manual, so I convert it into a factor with clearer labels. Therefore, these two variables form the foundation of both the observed comparison and the simulated null distribution.\nTo test these hypotheses, I use a permutation test. I compute the observed difference in mean MPG between manual and automatic cars, then simulate a world where transmission type has no relationship with MPG by repeatedly shuffling the transmission labels and recomputing the difference. This builds a null distribution for the statistic (Manual − Automatic mean MPG) that I can compare to the observed value to obtain a one sided p value.\n\ncars &lt;- as_tibble(mtcars, rownames = \"model\") |&gt;\n  mutate(am = factor(am, c(0,1), labels = c(\"Automatic\", \"Manual\")))\ncars |&gt;\n  head(n=5)\n\n# A tibble: 5 × 12\n  model          mpg   cyl  disp    hp  drat    wt  qsec    vs am     gear  carb\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0 Manu…     4     4\n2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0 Manu…     4     4\n3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1 Manu…     4     1\n4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1 Auto…     3     1\n5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0 Auto…     3     2\n\n\n\nggplot(cars, aes(x = am, y = mpg, fill = am)) +\n  geom_boxplot(width = 0.55, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1) +\n  labs(\n    title = \"Fuel economy (MPG) by transmission\",\n    x = \"Transmission\", y = \"MPG\", fill = \"Transmission\"\n    ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis boxplot compares fuel economy, MPG, between cars with automatic and manual transmissions in the “mtcars” dataset. In the plot, every dot represents one car, and from the boxplot we can see that manual cars generally have higher MPG than automatic cars, with both a higher median and a wider spread of values. Thus, this suggests that manual transmissions may be more fuel-efficent on average in this dataset.\nTo visualize the original data:\n\ncars |&gt;\n  group_by(am) |&gt;\n  summarise(\n    n = n(),\n    mean_mpg = mean(mpg),\n    median_mpg = median(mpg)\n  )\n\n# A tibble: 2 × 4\n  am            n mean_mpg median_mpg\n  &lt;fct&gt;     &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Automatic    19     17.1       17.3\n2 Manual       13     24.4       22.8\n\n\nNow, for the simulation part:\n\nmean_diff &lt;- function(data){\n  manual_mpg &lt;- mean(data$mpg[data$am == \"Manual\"])\n  auto_mpg   &lt;- mean(data$mpg[data$am == \"Automatic\"])\n  return(manual_mpg - auto_mpg)\n}\n\nperm_once &lt;- function(data){\n  data_perm &lt;- data\n  data_perm$am &lt;- sample(data_perm$am)\n  mean_diff(data_perm)\n}\n\nObserved difference in means:\n\nobs &lt;- mean_diff(cars)\nobs\n\n[1] 7.244939\n\n\nNull distribution under “no relationship” and one-side p-value:\n\nB &lt;- 5000\nnull_vals &lt;- map_dbl(1:B, function(i) perm_once(cars))\np_value &lt;- mean(null_vals &gt;= obs)\np_value_formatted &lt;- format(p_value, scientific = FALSE, digits = 4)\n\ntibble(\n  observed_diff = obs,\n  p_value_one_sided = p_value_formatted,\n  permutations = B\n)\n\n# A tibble: 1 × 3\n  observed_diff p_value_one_sided permutations\n          &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;\n1          7.24 0                         5000\n\n\nThe p_value_one_sided is actually equal to 0.0004, but R rounds it down to 0.\nDescription of the simulation:\nThe simulation works by repeatedly shuffling the transmission labels to represent an environment where there is no true relationship between transmission type and fuel economy (the null hypothesis). - The function mean_diff() calculates the difference in average MPG between manual and automatic cars for any given dataset. - The function perm_once() performs one permutation by randomly reassigning transmission labels and then calling mean_diff() to get a new difference value. - I used map_dbl() to repeat this process 5,000 times, because it provides a fast way to apply the same function many times and store the numeric results in a single vector. This created a simulated null distribution of MPG differences that we can compare the observed difference against to determine how different it is the under the null model.\nPlotting the null distribution:\n\ntibble(null = null_vals) |&gt;\n  ggplot(aes(x = null)) +\n  geom_histogram(bins = 40, alpha = 0.85) +\n  geom_vline(xintercept = obs, linetype = \"dashed\") +\n  labs(\n    title = \"Permutation null for MPG difference (Manual − Automatic)\",\n    x = \"Null statistic (Manual − Automatic mean MPG)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis histogram shows the null distribution of differences in mean MPG between manual and automatic cars, created by randomly shuffling the transmission labels 5,000 times. The x-axis represents the simulated differences under the null hypothesis of no relationship between transmission and MPG, with the y-axis showing how often each difference occurred. The dashed vertical line marks the observed difference from the real data, which was about 7.24 MPG. This observed value lies far to the right of all simulated values, and thus suggests that differences of this magnitude almost never appear in a world where transmission type has no relationship to MPG among early 1970s cars. So, this visual is consistnent with the small one sided p value and supports the idea that the observed difference is unlikely to be due to random variation alone.\n\n\nTo conclude:\nIn this analysis, I conducted a permutation test to determine whether cars with manual transmission have higher fuel economy, MPG, than cars with automatic transmission in the “mtcars” dataset. I computed the observed difference in mean MPG and then simulated the null hypothesis by repeatedly shuffling transmission labels and recalculating the mean difference 5,000 times. The resulting null distribution was centered around 0, while the observed difference (7.24 MPG) was far outside this range, giving a p-value of 0.0004, which is less than 0.01. This indicated that it is highly unlikely the difference in MPG occurred by chance, providing very strong evidence against the null model, and ultimately that manual cars are more fuel-efficient on average."
  },
  {
    "objectID": "kidzbop.html",
    "href": "kidzbop.html",
    "title": "Kidz Bop Censored Lyrics",
    "section": "",
    "text": "Dataset page: The Pudding – Kidz Bop Data\nOriginal source: Kidz Bop Censored Lyrics Dataset (Hehmeyer et al., The Pudding)\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)\n\n\nThe Kidz Bop censored lyrics dataset has one row for each censored instance in a song: the artist (ogArtist), song title (songName), a category for the type of content, the original lyric line (ogLyric), and the Kidz Bop version (kbLyric).\nBecause songs can have several censored words, and the same line can appear more than once, the same lyric line can show up in multiple rows. When I look at words or phrases inside ogLyric, I first collapse down to one row per unique original lyric line so I do not double count the same line.\n\nkb &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/kidz-bop/KB_censored-lyrics.csv\")\n\nkb_lines &lt;- kb |&gt; # collapsing to one row per unique ogLyric line\ndistinct(ogArtist, songName, ogLyric, .keep_all = TRUE)\n\n\nWhat does Kidz Bop censor most?\n\ncategory_counts &lt;- kb |&gt;\n  count(category)\n\ntop8 &lt;- head(category_counts, 8) \n\nggplot(top8, aes(x = reorder(category, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"What Kidz Bop censors most\",\n    subtitle = \"Top categories in the censored-lyrics dataset\",\n    x = \"Category\",\n    y = \"Censored instances\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot helps to show which types of content are most frequently censored in Kidz Bop songs. From the counts, we can see that profanity and sexual references make up the largest portion of censored lyrics, followed closely by alcohol and drug references. Categories like violence, identity, and other themes appear much less often.\nThese patterns reveal that Kidz Bop’s censorship choices focus mainly on removing language that could be considered inappropriate or too mature for a younger audience, while other categories are less frequently flagged. This highlights how producers are likely prioritizing a “family-friendly” tone in their songs.\n\n\nWords that follow “my” in lyric lines\n\nmy_after &lt;- kb_lines |&gt;\n  mutate(my_word = str_extract_all(ogLyric, \"(?&lt;=\\\\bmy\\\\s)\\\\w+\")) |&gt;\n  select(my_word) |&gt;\n  unnest(my_word) |&gt;\n  count(my_word, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\nmy_after\n\n# A tibble: 10 × 2\n   my_word     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 God        10\n 2 nights      7\n 3 body        4\n 4 bed         3\n 5 chest       3\n 6 exes        3\n 7 face        3\n 8 hand        3\n 9 head        3\n10 heart       3\n\n\nThis table uses kb_lines, so each original lyric line only appears once even if it was censored for multiple words. For all lines that contain the word “my”, it finds the word that immediately follows “my” and counts how often each one appears.\nThe most common followers of “my” include words like God, nights, and body, which hints at personal experiences, relationships, and emotional themes in the censored lines. This reflects the kinds of ideas that sometimes get edited in Kidz Bop versions.\n\n\nCommon openings of censored lyric lines\n\nfirst_three &lt;- kb_lines |&gt;\nmutate(first_three_words = str_extract(ogLyric, \"^(?:\\\\S+\\\\s+){0,2}\\\\S+\")) |&gt;\ncount(first_three_words, sort = TRUE) |&gt;\nslice_head(n = 10)\n\nfirst_three\n\n# A tibble: 10 × 2\n   first_three_words       n\n   &lt;chr&gt;               &lt;int&gt;\n 1 And I don't             4\n 2 Go out and              4\n 3 Ain't another woman     3\n 4 Fuck up my              3\n 5 I know that             3\n 6 I know you              3\n 7 I said I                3\n 8 Let's lose our          3\n 9 Lick your lips          3\n10 Shitting on y'all       3\n\n\nHere I look at the first three words of each unique original lyric line that contains censored content. This is about how censored lines start, not about the first line of each song. Repeated phrases can still show up because many songs repeat the same lyric line multiple times.\nMany of the common openings begin with short actions, direct expressions, or inappropriate words/romantic themes which then lead into content that Kidz Bop changes, such as romantic or suggestive wording.\n\n\nRelationship words over time\n\nkb_lower &lt;- kb |&gt;\n  filter(!is.na(year), !is.na(ogLyric))|&gt;\n  mutate(og_lower = str_to_lower(ogLyric))\n\nreg_expression &lt;- \"\\\\b(love|lover|kiss|kissing|baby|girlfriend|boyfriend|girl|boy|wife|husband|ex|partner)\\\\b\"\n\nby_year &lt;- kb_lower |&gt;\n  mutate(has_relationship = str_detect(og_lower, reg_expression)) |&gt;\n  group_by(year) |&gt;\n  summarise(\n    total = n(),\n    relationship = sum(has_relationship, na.rm = TRUE),\n    pct = mean(has_relationship, na.rm = TRUE)) |&gt;\n  filter(total &gt;= 20) |&gt; # helps to stabilize the percentages\n  ungroup()\n\nggplot(by_year, aes(x = year, y = pct)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))   +\n  labs(\n    title = \"Lyrics mentioning relationships over time\",\n    subtitle = \"Share of original lyrics with partner or family terms (years with 20+ rows)\",\n    x = \"Year\",\n    y = \"Percent of lyrics mentioning relationships\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plots shows how often relationship and love-related words such as love, kiss, baby, girlfriend, boyfriend, etc. appear in the original Kidz Bop lyrics over time. Although the percentages remain small overall, there are clear ups and downs across the years. At peaks around 2014-2015, the data suggests that more popular songs from those years contained romantic and affectionate language that Kidz Bop chose to censor and modify. On the other hand, lower points indicate years when songs with relationship themes were a little bit less common. Overall, we can see how the romantic tone of pop music can change over time and how Kidz Bop’s censorship accommodates those changing tones."
  }
]