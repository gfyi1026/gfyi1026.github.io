---
title: "Ethics - Data and Power - the COMPAS Algorithm"
description: "Analyzing the ethics in criminal risk prediction"
date: 2025-11-10
---

## Overview

COMPAS is a risk-assessment algorithm used by some US courts to estimate the likelihood that a defendant will reoffend. The tool was developed by Northpointe and is built on proprietary criminal-justice data that the public cannot access or evaluate (*Dieterich et al., 2016*). Courts use the resulting score as one factor in bail, sentencing, and supervision decisions. This is done with the hopes of limiting potential future crimes committed by these defendants.

More public scrutiny intensified after Angwin et al. (2016) analyzed COMPAS scores from Broward County and reported that Black defendants were more likely to be labeled high-risk but did not actually reoffend, while white defendants were more likely to be labeled low-risk but later did reoffend. Northpointe argued that the tool was well-calibrated overall and that differences arose from underlying disparities in the criminal-justice system.

These competing perspectives highlight how some algorithmic tools influence decisions about liberty and freedom without clear public understanding of how those tools operate.


## 1. What is the permission structure for using the data, and was it followed?

Northpointe licenses COMPAS to courts, which are permitted to enter defendants’ criminal histories, interview responses, and demographic information into the software (*Dieterich et al., 2016*). Because criminal-justice data are collected under legal mandate, courts have institutional permission to use this information.

Angwin et al. (2016) point out that individuals do not provide explicit consent for their personal records to be used in proprietary algorithms. So, we can conclude that the absence of consent is structural as defendants cannot opt out of having their information included. The permission structure reflects institutional needs rather than voluntary participation.


## 2. Who was measured, and are they representative of the population to whom the algorithm is applied?

Northpointe does not disclose the exact dataset used to train COMPAS. They state only that it reflects historical patterns from jurisdictions that purchased the tool (*Dieterich et al., 2016*).

The dataset analyzed by Angwin et al. (2016) includes people arrested in Broward County. This group reflects local policing and arrest patterns, which differ across communities. When COMPAS is applied broadly, many defendants scored by the algorithm may not resemble the individuals who informed the model.

This matters because the model inherits patterns from its training data, so overrepresentaiton of certain groups during data collection can lead to unequal outcomes during deployment.


## 3. Is the data being used in unintended ways relative to its original study?

Northpointe describes COMPAS as a decision-support tool rather than a replacement for judicial judgment (*Dieterich et al., 2016*). Yet Angwin et al. (2016) show that courts often treat the score as an authoritative measure of risk. In fast-paced environments, the score can become a stand-in for a more nuanced evaluation. This is an issue as judges and parole boards often treated the risk score as objective evidence.

This shift from advisory tool to decision substitute is an unintended use that shifts moral responsibility from humans to a an unclear algorithm. Because the model is proprietary, defendants cannot examine or contest the score that influences their case, which raises concerns about accountability and fairness.

## 4. Is the data identifiable or sufficiently anonymized?

COMPAS training data are private and undisclosed, so anonymization is not the central question. The ethical concern is the opacity of the model as neither the training data nor the variable weights can be independently reviewed.

The dataset used by Angwin et al. (2016) is different, where these records were obtained through public records requests and include defendants’ names, charges, and outcomes. Criminal justice records are by nature identifiable, and this data was not anonymized. The core privacy issue, therefore, is not anonymity but transparency. Without access to the model, the public cannot evaluate whether sensitive variables, or substitutes for those variables, were used.


## Why it matters

Algorithmic tools like COMPAS shift power in the criminal-justice process. Courts rely on a proprietary score that defendants cannot see, explain, or meaningfully challenge.This controversy matters because it reveals how power and accountability can shift when decision-making is handed over to algorithmic systems. In the justice system, where every decision carries serious personal consequences, even small biases can have lasting effects on the defendant's future. When a model like COMPAS produces a “risk score,” that number can shape how judges, prosecutors, and parole boards view a person’s future, which happens often without anyone fully understanding how the score was created.  

- **Who benefits:** Private vendors like Northpointe gain steady revenue from government contracts, and public agencies can also claim that their decisions are based on “objective” data rather than personal judgment. Courts would also gain a more streamlined workflow, and the appearance of neutrality would be appealing since it suggests consistency and fairness, even when that impression may not match reality.
- **Who is harmed:** Angwin et al. (2016) showed that defendants, especially Black defendants, are the ones most affected when the algorithm’s predictions are inaccurate. A higher risk score can mean stricter bail terms, longer sentences, or reduced chances of parole. Since the model is closed to public review, those individuals have no real way to challenge or verify the system that labels them. This results in a frustrating situation where the defendants are unfairly fighting against a biased system, in this case being COMPAS.
- **Why it matters ethically:** The use of predictive tools in criminal justice raises deep questions about transparency, fairness, and the distribution of power. These systems often reinforce existing biases rather than correcting them. When technology reinforces social bias under the cover of data-driven objectivity, it shifts power towards institutions that already hold power, and away from the people most impacted by their decisions. True fairness requires visibility and questioning whether efficiency and profit are being prioritized over justice.
---

## References

Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). *Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against Blacks.* ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing  

Dieterich, W., Mendoza, C., & Brennan, T. (2016). *COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity.* Northpointe. https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf
